{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2Core_Multiple_GPUs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty5I7hwOkacr",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, you are going to train a model via the `TF2.Core` APIs in in-depth replication with synchronous training on multiple GPUs. You are going to train a model on the FASHION MNIST dataset.\n",
        "\n",
        "References:\n",
        "* Custom training with tf.distribute.Strategy: https://www.tensorflow.org/tutorials/distribute/custom_training#training_loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiUck3bHerQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LBPQchMfGbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "c9c7d03d-8998-4efb-c513-dfdb95d05256"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"Tensorflow Version: {}\".format(tf.__version__))\n",
        "print(\"Eager Mode: {}\".format(tf.executing_eagerly()))\n",
        "print(\"GPU {} available.\".format(\"is\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not\"))\n",
        "print(\"List devices:\", device_lib.list_local_devices(), sep=\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow Version: 2.2.0-dev20200129\n",
            "Eager Mode: True\n",
            "GPU is available.\n",
            "List devices:\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 12662604050648042070\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 9344183571507787081\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13524239733876520926\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14912199066\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 615689992357562656\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2Hl5S99keDq",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj2eKqDrfh_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25dMBYUDklyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "731f7499-4390-4616-9411-f3eee5380a27"
      },
      "source": [
        "(train_imgs, train_labels), (test_imgs, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiapZvJBkw8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27e7c40b-9c50-45df-84de-90f004ae0c53"
      },
      "source": [
        "train_imgs.shape, train_labels.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK9AxXIsk42i",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, you are going to train a CNN model so that the image dataset is required to expand dimensions at the final axis as `(BATCH_SIZE, IMG_SIZE, IMG_SIZE, IMG_CHANNELS)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw1N8CWMkye8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3698cff6-bf62-4b9e-d41e-d7331d5d75c5"
      },
      "source": [
        "train_imgs = train_imgs[..., None]\n",
        "test_imgs = test_imgs[..., None]\n",
        "\n",
        "train_imgs.shape, test_imgs.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlbUVOJLllpD",
        "colab_type": "text"
      },
      "source": [
        "Always normalize the continuous data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So8o5cqzlkeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_imgs = train_imgs / np.float32(255.0)\n",
        "test_imgs = test_imgs / np.float32(255.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-8K8FbDl2ZA",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Distributed Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEnyWnAnEu-",
        "colab_type": "text"
      },
      "source": [
        "Next, you are going to create a distributed strategy via the `tf.distribute.MirroredStrategy` API. In this strategy, all variables are replicated to each replica, input pipeline is also distributed, each replica calculates the loss and the gradient itself, the gradients are synced across all replicas by summing them, and the update would be copied back to each replica after the sync."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmcTcdlal0nR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "640ef7a7-35d0-4e57-9199-e75f54f7d452"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOL1080cN3MJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b057ebb3-2d57-443f-fa47-b4c189188653"
      },
      "source": [
        "print(\"Number of devices: {}.\".format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOIVEdyOwk5",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ppKoGraN91m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(train_imgs)\n",
        "\n",
        "BUFFER_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BUFFER_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xfE9F01Pgqh",
        "colab_type": "text"
      },
      "source": [
        "You can simply access the dataset in memory via the `tf.data.Dataset.from_tensor_slices` APIs. After you created a `tf.data.Dataset`, you can make it a distributed pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeT95DRWPHZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSpxZEhxPf2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dist_datatset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d5GGRsR7tI",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBWSJtHRSdVV",
        "colab_type": "text"
      },
      "source": [
        "Here you can create a model via the `tf2.keras` APIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW8xAdZEQGf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  def _model_body(inputs):\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
        "                               activation='elu', name='input')(inputs)  # (None, 28, 28, 32)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)  # (None, 14, 14, 32)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
        "                               activation='elu')(x)  # (None, 14, 14, 64)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)  # (None, 7, 7, 64)\n",
        "    x = tf.keras.layers.Flatten()(x)  # (None, 7*7*64)\n",
        "    x = tf.keras.layers.Dense(units=64, activation='elu')(x)\n",
        "    output = tf.keras.layers.Dense(units=10, activation='softmax', name=\"output\")(x)\n",
        "    return output\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "  outputs = _model_body(inputs)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oft-GbO9US2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt_dir = \"./ckpts\"\n",
        "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59cJWsgOYTFO",
        "colab_type": "text"
      },
      "source": [
        "# Define the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbYE0Zg3cZcc",
        "colab_type": "text"
      },
      "source": [
        "In general, in one CPU/GPU device the loss value is divided by the number of examples of the input. However, if you train a model on the multiple GPU devices, you can first do the calculation that the loss value on each replica is divided by the `GLOBAL_BATCH_SIZE` (e.g. 16 batch sizes on 4 replicas, then the GLOBAL_BATCH_SIZE is 16*4=64). After the dividing calculation, you can sum all of them to the final loss value.\n",
        "\n",
        "In Tensorflow,\n",
        "* If you define a custom loss function, you can sum the per example losses on each replica and then divided by the GLOBAL_BATCH_SIZE. For example, `scaled_loss = tf.reduce_sum(losses) * (1.0 / GLOBAL_BATCH_SIZE)`. Or you can use `tf.nn.compute_acerage_loss`  which takes per example losses and GLOBAL_BATCH_SIZE as the arguments.\n",
        "\n",
        "* If you use the regularization loss, you need to scale the loss value by the number of replicas. (the `tf.nn.scale_regularization_loss` API)\n",
        "\n",
        "* If you use the `tf.keras.losses` classes, the loss reduction is required to set to one of `NONE` or `SUM`. (`AUTO` or `SUM_OVER_BATCH_SIZE` is not allowed.) The reduction and scaling are done automatically on the `compile` or `fit` step.\n",
        "\n",
        "* Do not use `tf.reduce_mean()` to calculate the loss value divided by the per replica batch size. This action causes the variation step by step.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BehS7BEaUh5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "  def compute_loss(labels, predictions):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    return tf.nn.compute_average_loss(per_example_loss=per_example_loss, \n",
        "                                      global_batch_size=GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Q-faz4eTG4",
        "colab_type": "text"
      },
      "source": [
        "# Define the Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S83OuQreBwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWv_ZjrcewcL",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO_d7v-1e5SR",
        "colab_type": "text"
      },
      "source": [
        "The model and optimizer must be under `strategy.scope`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFfjQzlXevsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  ckpts = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fSIY_Z6fTYE",
        "colab_type": "text"
      },
      "source": [
        "Define the train and test steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whs1UBxqfQf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images, training=True)\n",
        "      loss = compute_loss(labels, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "  def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PmKZIOhdxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "498f57d5-9873-4e1d-8153-ab1aa72fd767"
      },
      "source": [
        "with strategy.scope():\n",
        "  @tf.function\n",
        "  def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.experimental_run_v2(train_step, \n",
        "                                                      args=(dataset_inputs, ))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "  @tf.function\n",
        "  def distributed_test_step(dataset_inputs):\n",
        "    return strategy.experimental_run_v2(test_step, args=(dataset_inputs, ))\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    # train loop\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for x in train_dist_datatset:\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    # test loop\n",
        "    for x in test_dist_dataset:\n",
        "      distributed_test_step(x)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      ckpts.save(ckpt_prefix)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
        "                \"Test Accuracy: {}\")\n",
        "    print(template.format(epoch+1, train_loss, \n",
        "                          train_accuracy.result() * 100, \n",
        "                          test_loss.result(), \n",
        "                          test_accuracy.result() * 100))\n",
        "    \n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1, Loss: 0.41324031352996826, Accuracy: 85.2550048828125, Test Loss: 0.3330850601196289, Test Accuracy: 87.69000244140625\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 2, Loss: 0.2776425778865814, Accuracy: 90.07833862304688, Test Loss: 0.29328277707099915, Test Accuracy: 89.34000396728516\n",
            "Epoch 3, Loss: 0.23348034918308258, Accuracy: 91.4699935913086, Test Loss: 0.25261595845222473, Test Accuracy: 90.81999969482422\n",
            "Epoch 4, Loss: 0.20168009400367737, Accuracy: 92.63999938964844, Test Loss: 0.2690076231956482, Test Accuracy: 90.11000061035156\n",
            "Epoch 5, Loss: 0.17553049325942993, Accuracy: 93.44499969482422, Test Loss: 0.24960307776927948, Test Accuracy: 90.97000122070312\n",
            "Epoch 6, Loss: 0.14768004417419434, Accuracy: 94.52000427246094, Test Loss: 0.2564437687397003, Test Accuracy: 91.23999786376953\n",
            "Epoch 7, Loss: 0.12870216369628906, Accuracy: 95.18333435058594, Test Loss: 0.25209999084472656, Test Accuracy: 91.55999755859375\n",
            "Epoch 8, Loss: 0.1080591157078743, Accuracy: 95.96499633789062, Test Loss: 0.3281274139881134, Test Accuracy: 89.70000457763672\n",
            "Epoch 9, Loss: 0.09236881881952286, Accuracy: 96.61666870117188, Test Loss: 0.29860150814056396, Test Accuracy: 91.11000061035156\n",
            "Epoch 10, Loss: 0.07741028815507889, Accuracy: 97.1433334350586, Test Loss: 0.3174031376838684, Test Accuracy: 91.25999450683594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00wWK9NOllSG",
        "colab_type": "text"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSnoOSeYlrNl",
        "colab_type": "text"
      },
      "source": [
        "A model checkpoint can be loaded with or without a strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDyYDkWvksEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='eval_accuracy')\n",
        "\n",
        "new_model = create_model()\n",
        "new_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_imgs, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpQsK1bTmLEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def eval_step(images, labels):\n",
        "  predictions = new_model(images, training=False)\n",
        "  eval_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-izwXRmeDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf1a8cb4-5444-4f11-8c02-32aee316f713"
      },
      "source": [
        "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(ckpt_dir))\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "  eval_step(images, labels)\n",
        "\n",
        "print(\"Restoring the model without a strategy, the accuracy is {}.\".format(\n",
        "  eval_accuracy.result() * 100))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring the model without a strategy, the accuracy is 91.11000061035156.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3diz_FZrnMmw",
        "colab_type": "text"
      },
      "source": [
        "# Iterating Over a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGB--vonTCo",
        "colab_type": "text"
      },
      "source": [
        "## Using Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVPXqiFHnHP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c167b1df-f108-4a3f-b06b-d299f9052045"
      },
      "source": [
        "with strategy.scope():\n",
        "  for _ in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    train_iter = iter(train_dist_datatset)\n",
        "\n",
        "    for _ in range(10):\n",
        "      total_loss += distributed_train_step(next(train_iter))\n",
        "      num_batches += 1\n",
        "    average_train_loss = total_loss / num_batches\n",
        "\n",
        "    template = (\"Epoch {}, Loss {}, Accuracy {}\")\n",
        "    print (template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\n",
        "\n",
        "    train_accuracy.reset_states()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss 0.07053428143262863, Accuracy 97.96875\n",
            "Epoch 10, Loss 0.05302988365292549, Accuracy 98.28125\n",
            "Epoch 10, Loss 0.05932199954986572, Accuracy 98.59375\n",
            "Epoch 10, Loss 0.06402073800563812, Accuracy 96.875\n",
            "Epoch 10, Loss 0.05695180967450142, Accuracy 98.125\n",
            "Epoch 10, Loss 0.06418366730213165, Accuracy 96.5625\n",
            "Epoch 10, Loss 0.03831333667039871, Accuracy 98.125\n",
            "Epoch 10, Loss 0.060050249099731445, Accuracy 97.96875\n",
            "Epoch 10, Loss 0.06483839452266693, Accuracy 97.5\n",
            "Epoch 10, Loss 0.059599876403808594, Accuracy 97.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFs0Ml_ofcR",
        "colab_type": "text"
      },
      "source": [
        "## Iterating inside a TF.Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5KVW1adoWHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5ef1adbe-853a-4740-a771-aeb62b580ce4"
      },
      "source": [
        "with strategy.scope():\n",
        "  @tf.function\n",
        "  def distributed_train_epoch(dataset):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for x in dataset:\n",
        "      pre_replica_losses = strategy.experimental_run_v2(train_step, args=(x, ))\n",
        "      total_loss += strategy.reduce(\n",
        "        tf.distribute.ReduceOp.SUM, pre_replica_losses, axis=None)\n",
        "      num_batches += 1\n",
        "    return total_loss / tf.cast(num_batches, tf.float32)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_loss = distributed_train_epoch(train_dist_datatset)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
        "    print(template.format(epoch + 1, train_loss, train_accuracy.result() * 100))\n",
        "\n",
        "    train_accuracy.reset_states()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.06267233192920685, Accuracy: 97.68999481201172\n",
            "Epoch 2, Loss: 0.05286860093474388, Accuracy: 98.04499816894531\n",
            "Epoch 3, Loss: 0.04505863040685654, Accuracy: 98.35333251953125\n",
            "Epoch 4, Loss: 0.037907831370830536, Accuracy: 98.63333129882812\n",
            "Epoch 5, Loss: 0.03564491868019104, Accuracy: 98.66832733154297\n",
            "Epoch 6, Loss: 0.02910824678838253, Accuracy: 98.94000244140625\n",
            "Epoch 7, Loss: 0.027507269755005836, Accuracy: 98.99832916259766\n",
            "Epoch 8, Loss: 0.028171533718705177, Accuracy: 99.05500030517578\n",
            "Epoch 9, Loss: 0.021189115941524506, Accuracy: 99.27999877929688\n",
            "Epoch 10, Loss: 0.021836640313267708, Accuracy: 99.24666595458984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoXIg8qrjhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}