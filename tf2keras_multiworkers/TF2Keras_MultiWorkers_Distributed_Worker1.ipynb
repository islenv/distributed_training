{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPROwmKWfNpF"
   },
   "source": [
    "This tutorial guides you on how to use the `tf.distribute.Strategy` APIs to train a `tf2.keras` model on a multiple workers distributed architecture.\n",
    "\n",
    "References:\n",
    "* Multi-worker training with Keras: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n",
    "* Distributed Traning in Tensorflow: https://www.tensorflow.org/guide/distributed_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install -y iputils-ping net-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ifconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpypnnrEvLbz"
   },
   "outputs": [],
   "source": [
    "# !pip install -q tf-nightly\n",
    "# !pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "pVBsSJmfvgCP",
    "outputId": "4439619a-7bc3-47f4-b575-8daeb59f1138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "Eager Mode: True\n",
      "GPU not available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import json\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "print(\"Tensorflow Version: {}\".format(tf.__version__))\n",
    "print(\"Eager Mode: {}\".format(tf.executing_eagerly()))\n",
    "print(\"GPU {} available.\".format(\"is\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Workers Configuration\n",
    "\n",
    "In Tensorflow, `TF_CONFIG` environment variable is required for training on multiple machines, each of which has different roles. It is also used to specify the cluster configuration on each worker that is a part of the cluster.\n",
    "\n",
    "In `TF_CONFIG`, a JSON object, two main components are required, `cluster` and `task`. The `cluster` provides information about the training cluster, which is a dictionary consisting of different types of jobs such as `workers`. In general, a worker is the computing core for training. One of which is much special and is responsible for saving the checkpoints and other information for monitoring the progress in Tensorboard. This worker is also called a `chief` worker which is at index 0 by default.  The other `task` tag provides information about the current node.\n",
    "\n",
    "Each worker requires a copy of the `TF_CONFIG` environment variable.\n",
    "\n",
    "A simple example is below.\n",
    "\n",
    "```python\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': [\"localhost:12345\", \"localhost:23456\"]\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example to simulate the multiple workers' environment on the Docker containers.\n",
    "\n",
    "```bash\n",
    "docker pull tensorflow/tensorflow:latest-py3-jupyter\n",
    "\n",
    "# worker 1\n",
    "# ports:\n",
    "# |- 8888: jupyter notebook\n",
    "# |- 6006: Tensorboard\n",
    "# |- 12345: communicated with other workers\n",
    "docker run --net=bridge --name worker1 -p 8889:8888 -p 6007:6006 -p 12345:12345 tensorflow/tensorflow:latest-py3-jupyter\n",
    "\n",
    "# worker 2\n",
    "docker run --net=bridge --name worker2 -p 8890:8888 -p 12346:12346 tensorflow/tensorflow:latest-py3-jupyter\n",
    "```\n",
    "\n",
    "You can also use the docker-compose file (docker-compose.yml) to define the configuration of model training.\n",
    "\n",
    "```yml\n",
    "version: \"3\"\n",
    "services:\n",
    "  worker1:\n",
    "    image: tensorflow/tensorflow:latest-py3-jupyter\n",
    "    container_name: worker1\n",
    "    ports: \n",
    "      - \"8889:8888\"\n",
    "      - \"6007:6006\"\n",
    "      - \"12345:12345\"\n",
    "    volumes: \n",
    "      - \"/Users/jiankaiwang/devops/distributed_training:/tf/distributed_training\"\n",
    "  worker2:\n",
    "    image: tensorflow/tensorflow:latest-py3-jupyter\n",
    "    container_name: worker2\n",
    "    ports:\n",
    "      - \"8890:8888\"\n",
    "      - \"12346:12345\"\n",
    "    volumes: \n",
    "      - \"/Users/jiankaiwang/devops/distributed_training:/tf/distributed_training\"\n",
    "```\n",
    "\n",
    "You can start the docker containers.\n",
    "\n",
    "```sh\n",
    "# simple usage\n",
    "docker-compose -f docker-compose.yml up\n",
    "docker-compose -f docker-compose.yml ps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the example output.\n",
    "\n",
    "```sh\n",
    "# worker 1\n",
    "eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
    "        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255\n",
    "        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)\n",
    "        RX packets 3773  bytes 4867971 (4.8 MB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 2233  bytes 6349433 (6.3 MB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "\n",
    "# worker 2\n",
    "eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
    "        inet 172.17.0.3  netmask 255.255.0.0  broadcast 172.17.255.255\n",
    "        ether 02:42:ac:11:00:03  txqueuelen 0  (Ethernet)\n",
    "        RX packets 3704  bytes 4865869 (4.8 MB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 2043  bytes 6327067 (6.3 MB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': [\"172.19.0.2:12345\", \"172.19.0.3:12345\"]\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Right Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, multiple workers distributed training consists of two training types, synchronous and asynchronous ones. A synchronous training is synced on the steps across the workers and replicas. On the other side, An asynchronous one is not strictly on the steps.\n",
    "\n",
    "The `tf.distribute.experimental.MultiWorkerMirroredStrategy` API is the recommended way for synchronous multi-workers training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you see `untimeError: Collective ops must be configured at program startup`, try creating the instance of MultiWorkerMirroredStrategy at the beginning of the program and put the code that may create ops after the strategy is instantiated.** (Try to run the above script right after importing the Tensorflow library.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_ryAYrvfdby"
   },
   "source": [
    "# Preparing Datasets\n",
    "\n",
    "In this tutorial, you are going to build a CNN model on the FASHION MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "SMlJbEEgv9DI",
    "outputId": "89ea8cb2-3a15-4567-ad32-768ce1863767"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def make_datasets_unbatched(data_label='train', repeat=False):\n",
    "  def normalize(image, label):\n",
    "    img = tf.cast(image, tf.float32)\n",
    "    img /= 255.0\n",
    "    return img, label\n",
    "\n",
    "  datasets, info = tfds.load(name='fashion_mnist', with_info=True, as_supervised=True)\n",
    "  if repeat:\n",
    "    return datasets[data_label].repeat(repeat).map(normalize).cache().shuffle(BUFFER_SIZE)\n",
    "  else:\n",
    "    return datasets[data_label].map(normalize).cache().shuffle(BUFFER_SIZE)\n",
    "\n",
    "train_datasets = make_datasets_unbatched('train').batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8btYCYbUgiXT"
   },
   "source": [
    "# Build a TF2.Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIMuc09Rgfbw"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  def _model(inputs):\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='elu')(inputs)\n",
    "    x = tf.keras.layers.MaxPool2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(units=64, activation='elu')(x)\n",
    "    outputs = tf.keras.layers.Dense(units=10, activation='softmax', name='class_result')(x)\n",
    "    return outputs\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "  outputs = _model(inputs)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51xkg9kioNur"
   },
   "source": [
    "Let's try to train a model on a single device first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "lC0Pm4dan34O",
    "outputId": "01d187a7-0d9b-4f2f-914d-418bacaad67b"
   },
   "outputs": [],
   "source": [
    "single_worker_model = build_model()\n",
    "single_worker_model.fit(train_datasets, epochs=3, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model with MultiWorkerMirroredStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are going to integrate the `TF2.Keras` model with `tf.distribute.Strategy`, the only change is to enclose the model building and compiling (`model.compile()`) inside the scope of the strategy(`strategy.scope()`).\n",
    "\n",
    "Since `MultiWorkerMirroredStrategy` does not support last partial batch handling, pass the `steps_per_epoch` argument to ``model.fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you have to set up the data sharding policy to `DATA` or `OFF`. If `DATA` is set up, each worker would process the whole dataset and discard those not for itself. If `OFF` is set up, each worker would receive the whole dataset. However, if the `AUTO` is set up (by default), the training goes failed. The AUTO mode would first separate the data into several parts (`FILE` mode), so you have to make sure enough datasets provided. otherwise, an error occurred.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 2\n",
    "GLOBAL_BATCH_SIZE = 32 * NUM_WORKERS\n",
    "EPOCHS = 3\n",
    "\n",
    "with strategy.scope():\n",
    "  options = tf.data.Options()\n",
    "  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "  train_datasets = make_datasets_unbatched('train', EPOCHS).batch(GLOBAL_BATCH_SIZE)\n",
    "  train_datasets_no_auto_shard = train_datasets.with_options(options)  \n",
    "\n",
    "  # build and compile the model under the strategy scope\n",
    "  # Tensorflow would auto decides which devices the variables were placed.\n",
    "  multi_worker_model = build_model()\n",
    "    \n",
    "multi_worker_model.fit(train_datasets_no_auto_shard, epochs=EPOCHS, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Sharding and Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.distribute.Strategy` APIs would take care of data sharding automatically in multi-worker training during the `model.fit()`.\n",
    "\n",
    "You can also manual sharding for your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "options = tf.data.Options()\n",
    "\n",
    "# you can also set the policy to one of `OFF`, `AUTO`, `DATA`, `FILE`\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "\n",
    "train_datasets_no_auto_shard = train_datasets.with_options(options)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.distribute.Strategy` APIs come with advanced fault tolerance by preserving the training state. If one worker fails or unstable, the state would be recovered after it is reset. Such an advantage relies on the syncing of the global step. However, the training continues after the failing worker was recovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/keras-ckpt')]\n",
    "\n",
    "with strategy.scope():\n",
    "  multi_worker_ckpt_model = build_model()\n",
    "\n",
    "multi_worker_ckpt_model.fit(train_datasets_no_auto_shard, \n",
    "                            epochs=3, \n",
    "                            steps_per_epoch=5, \n",
    "                            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a worker gets preempted, the whole training progress pauses until the preempted worker is restarted. You can inspect the checkpoint files, the chief worker takes responsibility for the model saving and the other workers keep the temp training state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF2Keras_MultiWorkers_Distributed.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
